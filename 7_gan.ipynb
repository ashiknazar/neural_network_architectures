{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GANs)\n",
    "\n",
    "## Overview\n",
    "Generative Adversarial Networks (GANs) are a class of machine learning frameworks designed for generative modeling. They consist of two neural networks, a generator and a discriminator, that compete against each other, leading to the generation of new data samples that are indistinguishable from real data.\n",
    "\n",
    "## Architecture of GANs\n",
    "\n",
    "A GAN consists of two main components:\n",
    "\n",
    "1. **Generator (\\(G\\))**: The model that generates new data samples. It takes random noise as input and produces data samples.\n",
    "2. **Discriminator (\\(D\\))**: The model that distinguishes between real data samples and fake samples generated by the generator. It outputs the probability that a given sample is real.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "The GAN training process involves the following steps:\n",
    "\n",
    "1. **Generate Fake Data**: The generator creates fake data samples from random noise.\n",
    "2. **Discriminate**: The discriminator evaluates both real data and fake data, providing feedback on whether the samples are real or fake.\n",
    "3. **Update Models**: Both the generator and the discriminator are updated based on the loss functions.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "The objective of GANs can be represented as a minimax game:\n",
    "\n",
    "1. **Discriminator Loss**:\n",
    "   $$\n",
    "   \\mathcal{L}_D = -\\mathbb{E}_{x \\sim P_{\\text{data}}} [\\log D(x)] - \\mathbb{E}_{z \\sim P_z} [\\log (1 - D(G(z)))]\n",
    "   $$\n",
    "\n",
    "2. **Generator Loss**:\n",
    "   $$\n",
    "   \\mathcal{L}_G = -\\mathbb{E}_{z \\sim P_z} [\\log D(G(z))]\n",
    "   $$\n",
    "\n",
    "Where:\n",
    "- \\( P_{\\text{data}} \\) is the distribution of real data,\n",
    "- \\( P_z \\) is the distribution of random noise,\n",
    "- \\( G(z) \\) is the generated data,\n",
    "- \\( D(x) \\) is the discriminator's output for input \\( x \\).\n",
    "\n",
    "### Overall Objective\n",
    "\n",
    "The overall objective is to find the Nash equilibrium where both networks are optimized:\n",
    "$$\n",
    "\\min_G \\max_D \\mathcal{L}(D, G)\n",
    "$$\n",
    "\n",
    "## Use Cases of GANs\n",
    "\n",
    "GANs have a variety of applications, including:\n",
    "\n",
    "1. **Image Generation**:\n",
    "   - Creating realistic images from random noise (e.g., DCGAN).\n",
    "\n",
    "2. **Image-to-Image Translation**:\n",
    "   - Converting images from one domain to another (e.g., Pix2Pix, CycleGAN).\n",
    "\n",
    "3. **Super Resolution**:\n",
    "   - Enhancing the resolution of images (e.g., SRGAN).\n",
    "\n",
    "4. **Text-to-Image Synthesis**:\n",
    "   - Generating images based on textual descriptions (e.g., AttnGAN).\n",
    "\n",
    "5. **Anomaly Detection**:\n",
    "   - Identifying anomalies by training GANs on normal data.\n",
    "\n",
    "## Advantages of GANs\n",
    "\n",
    "- **High-Quality Generation**: GANs can generate highly realistic data samples.\n",
    "- **Flexibility**: They can be adapted to various tasks, such as image generation and domain adaptation.\n",
    "\n",
    "## Disadvantages of GANs\n",
    "\n",
    "- **Training Instability**: GANs can be difficult to train due to the adversarial nature of the competition.\n",
    "- **Mode Collapse**: The generator may produce a limited variety of outputs, failing to capture the diversity of the data.\n",
    "\n",
    "## Implementation in TensorFlow/Keras\n",
    "\n",
    "Hereâ€™s a basic example of how to implement a GAN in TensorFlow/Keras:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Generator model\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=100))\n",
    "    model.add(Dense(784, activation='tanh'))  # Example for MNIST (28x28)\n",
    "    return model\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation=LeakyReLU(alpha=0.2), input_dim=784))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Compile the models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Combine the models to create a GAN\n",
    "discriminator.trainable = False\n",
    "gan_input = tf.keras.Input(shape=(100,))\n",
    "generated_image = generator(gan_input)\n",
    "gan_output = discriminator(generated_image)\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "\n",
    "# Compile the GAN\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Example training loop (simplified)\n",
    "# for epoch in range(epochs):\n",
    "#     # Train the discriminator\n",
    "#     ...\n",
    "#     # Train the generator\n",
    "#     ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
