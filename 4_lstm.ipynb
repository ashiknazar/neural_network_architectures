{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM) Networks\n",
    "\n",
    "## Overview\n",
    "Long Short-Term Memory (LSTM) networks are a specialized type of Recurrent Neural Network (RNN) designed to better capture long-term dependencies in sequential data. They were introduced to address the limitations of standard RNNs, particularly the vanishing and exploding gradient problems.\n",
    "\n",
    "## Architecture of LSTM\n",
    "\n",
    "An LSTM unit consists of:\n",
    "1. **Cell State (\\(C_t\\))**: Carries the long-term memory of the network.\n",
    "2. **Hidden State (\\(h_t\\))**: Represents the short-term memory used for predictions.\n",
    "3. **Gates**: LSTMs have three main gates that control the flow of information:\n",
    "   - **Forget Gate (\\(f_t\\))**: Decides what information to discard from the cell state.\n",
    "   - **Input Gate (\\(i_t\\))**: Decides what new information to store in the cell state.\n",
    "   - **Output Gate (\\(o_t\\))**: Decides what information to output from the cell state.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "1. **Forget Gate**:\n",
    "   $$\n",
    "   f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "   $$\n",
    "\n",
    "2. **Input Gate**:\n",
    "   $$\n",
    "   i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "   $$\n",
    "\n",
    "3. **Candidate Cell State**:\n",
    "   $$\n",
    "   \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "   $$\n",
    "\n",
    "4. **Update Cell State**:\n",
    "   $$\n",
    "   C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t\n",
    "   $$\n",
    "\n",
    "5. **Output Gate**:\n",
    "   $$\n",
    "   o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "   $$\n",
    "\n",
    "6. **Hidden State**:\n",
    "   $$\n",
    "   h_t = o_t * \\tanh(C_t)\n",
    "   $$\n",
    "\n",
    "### Summary of LSTM Equations\n",
    "- Forget gate: \\( f_t \\)\n",
    "- Input gate: \\( i_t \\)\n",
    "- Candidate cell state: \\( \\tilde{C}_t \\)\n",
    "- Cell state update: \\( C_t \\)\n",
    "- Output gate: \\( o_t \\)\n",
    "- Hidden state: \\( h_t \\)\n",
    "\n",
    "## Use Cases of LSTM\n",
    "\n",
    "LSTMs are widely used in various applications, including:\n",
    "\n",
    "1. **Natural Language Processing (NLP)**:\n",
    "   - Language modeling\n",
    "   - Machine translation\n",
    "   - Sentiment analysis\n",
    "\n",
    "2. **Time Series Forecasting**:\n",
    "   - Predicting stock prices\n",
    "   - Weather forecasting\n",
    "\n",
    "3. **Speech Recognition**:\n",
    "   - Converting audio signals to text.\n",
    "\n",
    "4. **Music Generation**:\n",
    "   - Composing melodies based on previous notes.\n",
    "\n",
    "5. **Video Analysis**:\n",
    "   - Activity recognition in video streams.\n",
    "\n",
    "## Advantages of LSTM\n",
    "\n",
    "- **Long-Term Dependencies**: Capable of learning relationships between distant time steps in data.\n",
    "- **Gating Mechanisms**: The gates allow for fine-grained control over information flow, improving learning stability.\n",
    "\n",
    "## Disadvantages of LSTM\n",
    "\n",
    "- **Complexity**: LSTMs are more complex than standard RNNs, leading to longer training times and more parameters.\n",
    "- **Overfitting**: Due to their complexity, LSTMs may overfit on small datasets.\n",
    "\n",
    "## Implementation in TensorFlow/Keras\n",
    "\n",
    "Hereâ€™s a basic example of how to implement an LSTM in TensorFlow/Keras:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(timesteps, features)))  # 50 LSTM units\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
