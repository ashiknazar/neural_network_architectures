{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM) Networks\n",
    "\n",
    "## Overview\n",
    "Long Short-Term Memory (LSTM) networks are a specialized type of Recurrent Neural Network (RNN) designed to better capture long-term dependencies in sequential data. They were introduced to address the limitations of standard RNNs, particularly the vanishing and exploding gradient problems.\n",
    "\n",
    "## Architecture of LSTM\n",
    "\n",
    "An LSTM unit consists of:\n",
    "1. **Cell State (\\(C_t\\))**: Carries the long-term memory of the network.\n",
    "2. **Hidden State (\\(h_t\\))**: Represents the short-term memory used for predictions.\n",
    "3. **Gates**: LSTMs have three main gates that control the flow of information:\n",
    "   - **Forget Gate (\\(f_t\\))**: Decides what information to discard from the cell state.\n",
    "   - **Input Gate (\\(i_t\\))**: Decides what new information to store in the cell state.\n",
    "   - **Output Gate (\\(o_t\\))**: Decides what information to output from the cell state.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "1. **Forget Gate**:\n",
    "   $$\n",
    "   f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "   $$\n",
    "\n",
    "2. **Input Gate**:\n",
    "   $$\n",
    "   i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "   $$\n",
    "\n",
    "3. **Candidate Cell State**:\n",
    "   $$\n",
    "   \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "   $$\n",
    "\n",
    "4. **Update Cell State**:\n",
    "   $$\n",
    "   C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t\n",
    "   $$\n",
    "\n",
    "5. **Output Gate**:\n",
    "   $$\n",
    "   o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "   $$\n",
    "\n",
    "6. **Hidden State**:\n",
    "   $$\n",
    "   h_t = o_t * \\tanh(C_t)\n",
    "   $$\n",
    "\n",
    "### Summary of LSTM Equations\n",
    "- Forget gate: \\( f_t \\)\n",
    "- Input gate: \\( i_t \\)\n",
    "- Candidate cell state: \\( \\tilde{C}_t \\)\n",
    "- Cell state update: \\( C_t \\)\n",
    "- Output gate: \\( o_t \\)\n",
    "- Hidden state: \\( h_t \\)\n",
    "\n",
    "## Use Cases of LSTM\n",
    "\n",
    "LSTMs are widely used in various applications, including:\n",
    "\n",
    "1. **Natural Language Processing (NLP)**:\n",
    "   - Language modeling\n",
    "   - Machine translation\n",
    "   - Sentiment analysis\n",
    "\n",
    "2. **Time Series Forecasting**:\n",
    "   - Predicting stock prices\n",
    "   - Weather forecasting\n",
    "\n",
    "3. **Speech Recognition**:\n",
    "   - Converting audio signals to text.\n",
    "\n",
    "4. **Music Generation**:\n",
    "   - Composing melodies based on previous notes.\n",
    "\n",
    "5. **Video Analysis**:\n",
    "   - Activity recognition in video streams.\n",
    "\n",
    "## Advantages of LSTM\n",
    "\n",
    "- **Long-Term Dependencies**: Capable of learning relationships between distant time steps in data.\n",
    "- **Gating Mechanisms**: The gates allow for fine-grained control over information flow, improving learning stability.\n",
    "\n",
    "## Disadvantages of LSTM\n",
    "\n",
    "- **Complexity**: LSTMs are more complex than standard RNNs, leading to longer training times and more parameters.\n",
    "- **Overfitting**: Due to their complexity, LSTMs may overfit on small datasets.\n",
    "\n",
    "## Implementation in TensorFlow/Keras\n",
    "\n",
    "Hereâ€™s a basic example of how to implement an LSTM in TensorFlow/Keras:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(timesteps, features)))  # 50 LSTM units\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM) Gates and Their Functions\n",
    "\n",
    "LSTM (Long Short-Term Memory) is a type of Recurrent Neural Network (RNN) designed to overcome issues such as the vanishing gradient problem and to handle long-term dependencies in sequential data. LSTM cells are composed of several **gates** that control the flow of information into, out of, and within the memory cell. These gates allow the LSTM to remember important information over time, making it ideal for tasks like time series prediction, language modeling, and speech recognition.\n",
    "\n",
    "### Key Components of LSTM:\n",
    "\n",
    "1. **Forget Gate ($f_t$)**\n",
    "2. **Input Gate ($i_t$)**\n",
    "3. **Candidate Cell State ($\\tilde{C}_t$)**\n",
    "4. **Output Gate ($o_t$)**\n",
    "5. **Cell State ($C_t$)**\n",
    "6. **Hidden State ($h_t$)**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Forget Gate ($f_t$)\n",
    "\n",
    "The **forget gate** decides which information from the previous cell state ($C_{t-1}$) should be discarded. The output of the forget gate is a value between 0 and 1, where 0 means \"forget everything\" and 1 means \"keep everything\".\n",
    "\n",
    "#### Mathematical Equation:\n",
    "\\[\n",
    "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "\\]\n",
    "- $f_t$: Output of the forget gate\n",
    "- $\\sigma$: **Sigmoid activation function** (output between 0 and 1)\n",
    "- $W_f$: Weight matrix for the forget gate\n",
    "- $b_f$: Bias term for the forget gate\n",
    "- $h_{t-1}$: Hidden state from the previous timestep\n",
    "- $x_t$: Input at the current timestep\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Input Gate ($i_t$)\n",
    "\n",
    "The **input gate** controls how much new information from the candidate cell state ($\\tilde{C}_t$) should be added to the current memory cell state ($C_t$). The output of this gate is also a value between 0 and 1, indicating the degree of influence of the new information.\n",
    "\n",
    "#### Mathematical Equation:\n",
    "\\[\n",
    "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "\\]\n",
    "- $i_t$: Output of the input gate\n",
    "- $W_i$: Weight matrix for the input gate\n",
    "- $b_i$: Bias term for the input gate\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Candidate Cell State ($\\tilde{C}_t$)\n",
    "\n",
    "The **candidate cell state** represents the potential new memory that can be added to the cell state. It is computed by passing the current input and previous hidden state through a **tanh** activation function to ensure the values stay within a range of [-1, 1].\n",
    "\n",
    "#### Mathematical Equation:\n",
    "\\[\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "\\]\n",
    "- $\\tilde{C}_t$: Candidate cell state\n",
    "- $W_C$: Weight matrix for the candidate cell state\n",
    "- $b_C$: Bias term for the candidate cell state\n",
    "- $\\tanh$: **Hyperbolic tangent activation function**\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Output Gate ($o_t$)\n",
    "\n",
    "The **output gate** decides what the next hidden state ($h_t$) should be, based on the current cell state ($C_t$). This gate filters the cell state by applying the **sigmoid** function and then passes it through a **tanh** activation to produce the final output.\n",
    "\n",
    "#### Mathematical Equation:\n",
    "\\[\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "\\]\n",
    "- $o_t$: Output of the output gate\n",
    "- $W_o$: Weight matrix for the output gate\n",
    "- $b_o$: Bias term for the output gate\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Cell State Update ($C_t$)\n",
    "\n",
    "The **cell state** ($C_t$) is updated at each timestep by combining the previous cell state ($C_{t-1}$) and the new candidate cell state ($\\tilde{C}_t$), using the forget and input gates. The forget gate controls how much of the previous state should be retained, while the input gate determines how much of the new candidate state should be added.\n",
    "\n",
    "#### Mathematical Equation:\n",
    "\\[\n",
    "C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n",
    "\\]\n",
    "- $C_t$: Updated cell state\n",
    "- $C_{t-1}$: Previous cell state\n",
    "- $f_t$: Forget gate output\n",
    "- $i_t$: Input gate output\n",
    "- $\\tilde{C}_t$: Candidate cell state\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Hidden State ($h_t$)\n",
    "\n",
    "The **hidden state** ($h_t$) is the output of the LSTM cell and is used for the next timestep as well as the final output. It is computed by applying the output gate to the updated cell state ($C_t$).\n",
    "\n",
    "#### Mathematical Equation:\n",
    "\\[\n",
    "h_t = o_t \\cdot \\tanh(C_t)\n",
    "\\]\n",
    "- $h_t$: Hidden state (output of the LSTM cell)\n",
    "- $o_t$: Output gate\n",
    "- $C_t$: Updated cell state\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of LSTM Equations:\n",
    "\n",
    "1. **Forget Gate**:\n",
    "   \\[\n",
    "   f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "   \\]\n",
    "\n",
    "2. **Input Gate**:\n",
    "   \\[\n",
    "   i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "   \\]\n",
    "\n",
    "3. **Candidate Cell State**:\n",
    "   \\[\n",
    "   \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "   \\]\n",
    "\n",
    "4. **Cell State Update**:\n",
    "   \\[\n",
    "   C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n",
    "   \\]\n",
    "\n",
    "5. **Output Gate**:\n",
    "   \\[\n",
    "   o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "   \\]\n",
    "\n",
    "6. **Hidden State**:\n",
    "   \\[\n",
    "   h_t = o_t \\cdot \\tanh(C_t)\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "LSTM networks solve the vanishing gradient problem in traditional RNNs by incorporating special gates that control the flow of information. The **forget gate** decides what to discard from the memory, the **input gate** controls what new information to add, and the **output gate** determines what the network should output. The **cell state** is the key to maintaining long-term memory, while the **hidden state** is the immediate output used in subsequent timesteps.\n",
    "\n",
    "By using these gates and their mathematical functions, LSTM networks are capable of learning long-term dependencies, making them effective for tasks involving sequential data such as time series forecasting, natural language processing, and speech recognition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
