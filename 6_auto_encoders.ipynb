{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](auto_encoders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](autoencoder_schema.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "## Overview\n",
    "Autoencoders are a type of neural network used for unsupervised learning. They are designed to learn efficient representations of input data by encoding it into a lower-dimensional space and then decoding it back to reconstruct the original input. Autoencoders are widely used for dimensionality reduction, denoising, and feature learning.\n",
    "\n",
    "## Architecture of Autoencoders\n",
    "\n",
    "An autoencoder consists of two main components:\n",
    "\n",
    "1. **Encoder**: The part of the network that compresses the input into a lower-dimensional representation.\n",
    "2. **Decoder**: The part that reconstructs the input from the compressed representation.\n",
    "\n",
    "### Components of an Autoencoder\n",
    "\n",
    "- **Input Layer**: The layer that receives the original input data.\n",
    "- **Hidden Layer (Latent Space)**: The layer where the compressed representation is stored. This layer has fewer neurons than the input layer.\n",
    "- **Output Layer**: The layer that outputs the reconstructed input.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "The process can be described mathematically as follows:\n",
    "\n",
    "1. **Encoding**:\n",
    "   $$\n",
    "   h = f(W_e \\cdot x + b_e)\n",
    "   $$\n",
    "   Where:\n",
    "   - \\( h \\) is the encoded representation (latent space).\n",
    "   - \\( W_e \\) is the weight matrix for the encoder.\n",
    "   - \\( b_e \\) is the bias for the encoder.\n",
    "   - \\( f \\) is an activation function (commonly ReLU or sigmoid).\n",
    "\n",
    "2. **Decoding**:\n",
    "   $$\n",
    "   \\hat{x} = g(W_d \\cdot h + b_d)\n",
    "   $$\n",
    "   Where:\n",
    "   - \\( \\hat{x} \\) is the reconstructed input.\n",
    "   - \\( W_d \\) is the weight matrix for the decoder.\n",
    "   - \\( b_d \\) is the bias for the decoder.\n",
    "   - \\( g \\) is typically the same activation function used in the encoder.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The loss function measures the difference between the original input \\( x \\) and the reconstructed input \\( \\hat{x} \\). A common choice is the Mean Squared Error (MSE):\n",
    "$$\n",
    "L = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2\n",
    "$$\n",
    "\n",
    "## Use Cases of Autoencoders\n",
    "\n",
    "Autoencoders have various applications, including:\n",
    "\n",
    "1. **Dimensionality Reduction**:\n",
    "   - Reducing the number of features while preserving important information.\n",
    "\n",
    "2. **Denoising**:\n",
    "   - Removing noise from data by training on noisy inputs and clean outputs.\n",
    "\n",
    "3. **Anomaly Detection**:\n",
    "   - Identifying unusual patterns in data by comparing reconstruction errors.\n",
    "\n",
    "4. **Image Compression**:\n",
    "   - Compressing images into smaller representations while maintaining quality.\n",
    "\n",
    "5. **Feature Learning**:\n",
    "   - Learning meaningful representations from data that can be used for other tasks.\n",
    "\n",
    "## Advantages of Autoencoders\n",
    "\n",
    "- **Unsupervised Learning**: They can learn from unlabeled data, making them versatile for various tasks.\n",
    "- **Feature Extraction**: Capable of discovering hidden patterns and features in data.\n",
    "\n",
    "## Disadvantages of Autoencoders\n",
    "\n",
    "- **Reconstruction Limitations**: The quality of the reconstruction depends heavily on the architecture and training process.\n",
    "- **Overfitting**: They can easily overfit, especially when the model is too complex relative to the amount of training data.\n",
    "\n",
    "## Implementation in TensorFlow/Keras\n",
    "\n",
    "Hereâ€™s a basic example of how to implement an autoencoder in TensorFlow/Keras:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the size of the input and the latent space\n",
    "input_dim = 784  # Example for MNIST (28x28)\n",
    "latent_dim = 32\n",
    "\n",
    "# Define the encoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(latent_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Define the decoder\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Fit the model (example training data: X_train)\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, shuffle=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
