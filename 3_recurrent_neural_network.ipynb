{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](rnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "\n",
    "## Overview\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed for processing sequential data. Unlike traditional feedforward networks, RNNs have connections that loop back on themselves, allowing them to maintain a state or memory of previous inputs.\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "1. **Basic Structure**: \n",
    "   - An RNN consists of an input layer, hidden layer(s), and output layer. The key feature is the recurrent connection in the hidden layer.\n",
    "   \n",
    "2. **Mathematical Representation**:\n",
    "   - Given an input sequence \\( x = (x_1, x_2, \\ldots, x_T) \\), the hidden state \\( h_t \\) at time step \\( t \\) is computed as:\n",
    "   $$\n",
    "   h_t = \\sigma(W_h h_{t-1} + W_x x_t + b)\n",
    "   $$\n",
    "   where:\n",
    "   - \\( W_h \\) is the weight matrix for the hidden state,\n",
    "   - \\( W_x \\) is the weight matrix for the input,\n",
    "   - \\( b \\) is the bias,\n",
    "   - \\( \\sigma \\) is an activation function (often \\( \\tanh \\) or ReLU).\n",
    "   \n",
    "3. **Output Calculation**:\n",
    "   - The output \\( y_t \\) at time step \\( t \\) can be computed as:\n",
    "   $$\n",
    "   y_t = W_y h_t + b_y\n",
    "   $$\n",
    "   where \\( W_y \\) is the weight matrix for the output layer and \\( b_y \\) is the output bias.\n",
    "\n",
    "4. **Training with Backpropagation Through Time (BPTT)**:\n",
    "   - The RNN is trained using BPTT, which unfolds the network across time steps and applies backpropagation to update weights.\n",
    "   - The loss function (often mean squared error for regression or cross-entropy for classification) is computed, and gradients are propagated back through time.\n",
    "\n",
    "## Learning Mechanism\n",
    "- RNNs learn to predict the next item in a sequence based on previous items by adjusting weights to minimize the loss.\n",
    "- They capture temporal dependencies by maintaining a hidden state that is updated at each time step.\n",
    "\n",
    "## Applications\n",
    "RNNs are used in various applications, including:\n",
    "\n",
    "1. **Natural Language Processing (NLP)**:\n",
    "   - Language modeling\n",
    "   - Sentiment analysis\n",
    "   - Machine translation\n",
    "   - Text generation\n",
    "\n",
    "2. **Speech Recognition**:\n",
    "   - Converting spoken language into text.\n",
    "\n",
    "3. **Time Series Prediction**:\n",
    "   - Forecasting stock prices, weather, etc.\n",
    "\n",
    "4. **Music Generation**:\n",
    "   - Composing music based on previous notes.\n",
    "\n",
    "5. **Video Analysis**:\n",
    "   - Activity recognition in video sequences.\n",
    "\n",
    "## Advantages\n",
    "- **Temporal Dynamics**: RNNs are excellent at handling sequences of varying lengths and capturing temporal relationships.\n",
    "- **Parameter Sharing**: The same weights are applied at each time step, leading to a smaller model size.\n",
    "\n",
    "## Disadvantages\n",
    "- **Vanishing and Exploding Gradients**: Training can be unstable due to the vanishing or exploding gradient problems, particularly in long sequences.\n",
    "- **Long-Term Dependencies**: Standard RNNs struggle to learn long-range dependencies, which is addressed by architectures like LSTMs and GRUs.\n",
    "- **Training Time**: Training can be slower due to the sequential nature of the data.\n",
    "\n",
    "## Variants\n",
    "1. **LSTM (Long Short-Term Memory)**: Designed to better capture long-range dependencies with special gating mechanisms.\n",
    "2. **GRU (Gated Recurrent Unit)**: A simpler alternative to LSTMs with fewer parameters, retaining the ability to learn long-term dependencies.\n",
    "\n",
    "## Conclusion\n",
    "RNNs are powerful tools for sequence prediction and analysis, particularly in fields like NLP and time series forecasting. While they have limitations, advancements like LSTMs and GRUs have enhanced their capabilities, making them a cornerstone of modern deep learning architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](rnn_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN) Overview\n",
    "\n",
    "The diagram represents the structure of a Recurrent Neural Network (RNN), where the same function (denoted as \"A\") is applied across different time steps to process sequential data.\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "- **A**: The RNN unit, which is the same across all time steps. It processes both the input at the current time step and the hidden state from the previous time step.\n",
    "- **\\( h_t \\)**: The hidden state at time step \\( t \\). It is a function of:\n",
    "  - The input \\( x_t \\) at the current time step.\n",
    "  - The hidden state \\( h_{t-1} \\) from the previous time step.\n",
    "  The hidden state captures information from both the current input and previous time steps.\n",
    "- **\\( x_t \\)**: The input at time step \\( t \\).\n",
    "\n",
    "#### How it works:\n",
    "- The RNN unit processes each input \\( x_t \\) sequentially.\n",
    "- The loop inside each RNN unit represents the recurrent nature of the network, where information from the previous time step is used in the current step.\n",
    "- This allows the network to maintain a memory of previous inputs, making it ideal for tasks involving sequences, like time-series prediction and natural language processing.\n",
    "\n",
    "#### Example Use Cases:\n",
    "- Time-series data (e.g., stock prices, weather data)\n",
    "- Natural language processing (e.g., text generation, translation)\n",
    "- Sequence classification and prediction tasks\n",
    "\n",
    "#### RNN Computation:\n",
    "At each time step \\( t \\), the following happens:\n",
    "\\[\n",
    "h_t = f(W_h \\cdot h_{t-1} + W_x \\cdot x_t)\n",
    "\\]\n",
    "Where:\n",
    "- \\( f \\) is a non-linear activation function (e.g., tanh, ReLU).\n",
    "- \\( W_h \\) and \\( W_x \\) are weight matrices for the hidden state and input, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN) with Input Sequence Example\n",
    "\n",
    "## Input Sequence\n",
    "Let‚Äôs define our input sequence:\n",
    "- \\( x_1 = a \\)\n",
    "- \\( x_2 = b \\)\n",
    "- \\( x_3 = c \\)\n",
    "- \\( x_4 = d \\)\n",
    "- \\( x_5 = e \\)\n",
    "- \\( x_6 = f \\)\n",
    "\n",
    "## RNN Processing Steps\n",
    "\n",
    "1. **Initialization**: \n",
    "   - At the start, we initialize the hidden state \\( h_0 \\) (usually set to zeros).\n",
    "\n",
    "2. **Processing Each Input**:\n",
    "   - For each time step \\( t \\), the RNN updates its hidden state based on the current input \\( x_t \\) and the previous hidden state \\( h_{t-1} \\).\n",
    "\n",
    "3. **Hidden State Update**:\n",
    "   - For each input, the hidden state \\( h_t \\) is computed using the formula:\n",
    "   $$\n",
    "   h_t = \\sigma(W_h h_{t-1} + W_x x_t + b)\n",
    "   $$\n",
    "   where:\n",
    "   - \\( W_h \\) is the weight matrix for the hidden state,\n",
    "   - \\( W_x \\) is the weight matrix for the input,\n",
    "   - \\( b \\) is the bias,\n",
    "   - \\( \\sigma \\) is the activation function.\n",
    "\n",
    "## Step-by-Step Calculation\n",
    "\n",
    "Let‚Äôs illustrate this step-by-step for our sequence:\n",
    "\n",
    "- **For \\( t = 1 \\)** (input \\( x_1 = a \\)):\n",
    "  $$\n",
    "  h_1 = \\sigma(W_h h_0 + W_x a + b)\n",
    "  $$\n",
    "\n",
    "- **For \\( t = 2 \\)** (input \\( x_2 = b \\)):\n",
    "  $$\n",
    "  h_2 = \\sigma(W_h h_1 + W_x b + b)\n",
    "  $$\n",
    "\n",
    "- **For \\( t = 3 \\)** (input \\( x_3 = c \\)):\n",
    "  $$\n",
    "  h_3 = \\sigma(W_h h_2 + W_x c + b)\n",
    "  $$\n",
    "\n",
    "- **For \\( t = 4 \\)** (input \\( x_4 = d \\)):\n",
    "  $$\n",
    "  h_4 = \\sigma(W_h h_3 + W_x d + b)\n",
    "  $$\n",
    "\n",
    "- **For \\( t = 5 \\)** (input \\( x_5 = e \\)):\n",
    "  $$\n",
    "  h_5 = \\sigma(W_h h_4 + W_x e + b)\n",
    "  $$\n",
    "\n",
    "- **For \\( t = 6 \\)** (input \\( x_6 = f \\)):\n",
    "  $$\n",
    "  h_6 = \\sigma(W_h h_5 + W_x f + b)\n",
    "  $$\n",
    "\n",
    "## Output Calculation\n",
    "After processing all inputs, the RNN can produce outputs based on the last hidden state or at each time step:\n",
    "\n",
    "- For the output at time step \\( t \\):\n",
    "$$\n",
    "y_t = W_y h_t + b_y\n",
    "$$\n",
    "\n",
    "Where \\( W_y \\) is the weight matrix for the output layer and \\( b_y \\) is the output bias.\n",
    "\n",
    "## Summary\n",
    "In this way, the RNN processes the entire sequence \\( (a, b, c, d, e, f) \\) by updating its hidden state at each time step, allowing it to learn from the context provided by previous inputs. This mechanism enables RNNs to capture temporal dependencies in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a sequence a sequence of data that has a defined order.\n",
    "- rnn can transform sequences to vectors ,vectors to sequence ,sequences to sequences\n",
    "- dynamical systems\n",
    "    1. know the state of a system now at time t.what will be state at time t+n\n",
    "    2. (stock prediction)\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deeplearning?<br>\n",
    "method of representing differentiable functions that maps a variable of one type to a variable of another type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(in\\_var)=out\\_var $$\n",
    " $$f_{reg}:{R^D} ->R$$\n",
    " $$f_{classification}:{R^D} -> R^c $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectors are matrices of abstraction of raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a sequence is a sequence of data that has an order\n",
    "  - rnn can transform sequence to vectors ,vectors to sequences and sequences to sequences\n",
    "- dynamical system\n",
    "   - Unlike feedforward neural networks, which process inputs independently, RNNs have loops that allow them to maintain a hidden state that can capture temporal dependencies.\n",
    "   - predicting future out\n",
    "   - let S^t= state of a system at time t ,we can say that state of system at t+1 is s^(t+1)= f(s^t;theta)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](rnn_3.png)\n",
    "![Alt text](rnn_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- maps a sequence 'X' to another sequence 'O\n",
    "- amount of information retained is determined by the Weight W from previous timestep\n",
    "- h(t) is \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a^{(t)} = W h^{(t-1)} + U x^{(t)} + b_u\n",
    "$$\n",
    "\n",
    "$$\n",
    "h^{(t)} = \\tanh(a^{(t)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h^{(t)} \\text{ same as } h^{(t+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x^{(t)} \\text{ not same as } x^{(t+1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c^{(t)} = V h^{(t)} + b_v\n",
    "$$\n",
    "\n",
    "$$\n",
    "o^{(t)} = \\text{softmax}(c^{(t)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "L^{(t)} = \\text{Loss}(o^{(t)}, y^{(t)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = \\sum_{t} L^{(t)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden state \n",
    "\n",
    "h \n",
    "(t)\n",
    "  is recurrent and is meant to capture the internal memory of the network. This memory persists across different time steps, which means it is updated but carries information from previous time steps. Therefore, \n",
    "\n",
    "h \n",
    "(t)\n",
    "  and \n",
    "\n",
    "h \n",
    "(t+1)\n",
    "  are related‚Äîthey are not independent. The value of \n",
    "\n",
    "h \n",
    "(t+1)\n",
    "  depends on \n",
    "\n",
    "h \n",
    "(t)\n",
    " , but it gets updated as new input is processed.\n",
    "While they are not exactly the same at different time steps, they are connected in a chain and retain information from prior states.\n",
    "Input \n",
    "\n",
    "x \n",
    "(t)\n",
    " :\n",
    "\n",
    "The input \n",
    "\n",
    "x \n",
    "(t)\n",
    "  represents the data fed into the network at each time step \n",
    "ùë°\n",
    "t. Unlike \n",
    "\n",
    "h \n",
    "(t)\n",
    " , the input at each time step is typically independent of other inputs. Therefore, \n",
    "\n",
    "x \n",
    "(t)\n",
    "  and \n",
    "\n",
    "x \n",
    "(t+1)\n",
    "  are not the same, as the data points or observations at these time steps can vary and are not connected like the hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back Propagation Through time(bptt)\n",
    "- at every time step we need to perform back propagation \n",
    "   - large cost\n",
    "- teacher forcing\n",
    "   - instead of feeding hidden layer of previous state to hidden layer of next state ,we feed output y from previous state\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](rnn_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- predictions could be off ,because in training we passed actual y,in but real world problem is different scinareo\n",
    "- BPTT is a method for training RNNs by calculating gradients across time steps.\n",
    "Teacher Forcing is a strategy during training where the actual target outputs are used to guide the RNN, rather than its own predictions.\n",
    "- another architecture is sequence to vector model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![Alt text](rnn_10.png)\n",
    " ![Alt text](rnn_11.png)\n",
    " ![Alt text](rnn_12.png)\n",
    " ![Alt text](rnn_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- language translation is not just word to word conversion,words coming next may also affect the translation\n",
    "- text summarization and language translation dont use rnn in this specific architecture (here length of input sequence need to be length of output sequence)\n",
    "  - so use bidirectional architecture with unequal input and output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](rnn_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "import itertools\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import operator\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\ashik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
